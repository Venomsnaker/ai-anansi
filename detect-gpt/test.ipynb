{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import custom_datasets\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model():\n",
    "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        mask_model.cpu()\n",
    "    except NameError:\n",
    "        pass\n",
    "    if args.openai_model is None:\n",
    "        base_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "\n",
    "def load_mask_model():\n",
    "    print('MOVING MASK MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "\n",
    "    if args.openai_model is None:\n",
    "        base_model.cpu()\n",
    "    if not args.random_fills:\n",
    "        mask_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize, Mask & Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text, span_length, pct, ceil_pct=False):\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    n_spans = pct * len(tokens) / (span_length + args.buffer_size * 2)\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - args.buffer_size)\n",
    "        search_end = min(len(tokens), end + args.buffer_size)\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def count_masks(texts):\n",
    "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "\n",
    "\n",
    "# replace each masked span with a sample from T5 mask_model\n",
    "def replace_masks(texts):\n",
    "    n_expected = count_masks(texts)\n",
    "    stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "    tokens = mask_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs = mask_model.generate(**tokens, max_length=150, do_sample=True, top_p=args.mask_top_p, num_return_sequences=1, eos_token_id=stop_id)\n",
    "    return mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fills(texts):\n",
    "    # remove <pad> from beginning of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # return the text in between each matched mask token\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # remove whitespace around each fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "\n",
    "def apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # replace each mask token with the corresponding fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # join tokens back into text\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_texts_(texts, span_length, pct, ceil_pct=False):\n",
    "    if not args.random_fills:\n",
    "        masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
    "        raw_fills = replace_masks(masked_texts)\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "\n",
    "        # Handle the fact that sometimes the model doesn't generate the right number of fills and we have to try again\n",
    "        attempts = 1\n",
    "        while '' in perturbed_texts:\n",
    "            idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
    "            print(f'WARNING: {len(idxs)} texts have no fills. Trying again [attempt {attempts}].')\n",
    "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for idx, x in enumerate(texts) if idx in idxs]\n",
    "            raw_fills = replace_masks(masked_texts)\n",
    "            extracted_fills = extract_fills(raw_fills)\n",
    "            new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "            for idx, x in zip(idxs, new_perturbed_texts):\n",
    "                perturbed_texts[idx] = x\n",
    "            attempts += 1\n",
    "    else:\n",
    "        if args.random_fills_tokens:\n",
    "            # tokenize base_tokenizer\n",
    "            tokens = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            valid_tokens = tokens.input_ids != base_tokenizer.pad_token_id\n",
    "            replace_pct = args.pct_words_masked * (args.span_length / (args.span_length + 2 * args.buffer_size))\n",
    "\n",
    "            # replace replace_pct of input_ids with random tokens\n",
    "            random_mask = torch.rand(tokens.input_ids.shape, device=DEVICE) < replace_pct\n",
    "            random_mask &= valid_tokens\n",
    "            random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
    "            # while any of the random tokens are special tokens, replace them with random non-special tokens\n",
    "            while any(base_tokenizer.decode(x) in base_tokenizer.all_special_tokens for x in random_tokens):\n",
    "                random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
    "            tokens.input_ids[random_mask] = random_tokens\n",
    "            perturbed_texts = base_tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=True)\n",
    "        else:\n",
    "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
    "            perturbed_texts = masked_texts\n",
    "            # replace each <extra_id_*> with args.span_length random words from FILL_DICTIONARY\n",
    "            for idx, text in enumerate(perturbed_texts):\n",
    "                filled_text = text\n",
    "                for fill_idx in range(count_masks([text])[0]):\n",
    "                    fill = random.sample(FILL_DICTIONARY, span_length)\n",
    "                    filled_text = filled_text.replace(f\"<extra_id_{fill_idx}>\", \" \".join(fill))\n",
    "                assert count_masks([filled_text])[0] == 0, \"Failed to replace all masks\"\n",
    "                perturbed_texts[idx] = filled_text\n",
    "\n",
    "    return perturbed_texts\n",
    "\n",
    "\n",
    "def perturb_texts(texts, span_length, pct, ceil_pct=False):\n",
    "    chunk_size = args.chunk_size\n",
    "    if '11b' in mask_filling_model_name:\n",
    "        chunk_size //= 2\n",
    "\n",
    "    outputs = []\n",
    "    for i in tqdm.tqdm(range(0, len(texts), chunk_size), desc=\"Applying perturbations\"):\n",
    "        outputs.extend(perturb_texts_(texts[i:i + chunk_size], span_length, pct, ceil_pct=ceil_pct))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_last_word(text):\n",
    "    return ' '.join(text.split(' ')[:-1])\n",
    "\n",
    "def _openai_sample(p):\n",
    "    if args.dataset != 'pubmed':  # keep Answer: prefix for pubmed\n",
    "        p = drop_last_word(p)\n",
    "\n",
    "    # sample from the openai model\n",
    "    kwargs = { \"engine\": args.openai_model, \"max_tokens\": 200 }\n",
    "    if args.do_top_p:\n",
    "        kwargs['top_p'] = args.top_p\n",
    "    \n",
    "    r = openai.Completion.create(prompt=f\"{p}\", **kwargs)\n",
    "    return p + r['choices'][0].text\n",
    "\n",
    "# sample from base_model using ****only**** the first 30 tokens in each example as context\n",
    "def sample_from_model(texts, min_words=55, prompt_tokens=30):\n",
    "    # encode each text as a list of token ids\n",
    "    if args.dataset == 'pubmed':\n",
    "        texts = [t[:t.index(custom_datasets.SEPARATOR)] for t in texts]\n",
    "        all_encoded = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    else:\n",
    "        all_encoded = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}\n",
    "\n",
    "    if args.openai_model:\n",
    "        # decode the prefixes back into text\n",
    "        prefixes = base_tokenizer.batch_decode(all_encoded['input_ids'], skip_special_tokens=True)\n",
    "        pool = ThreadPool(args.batch_size)\n",
    "\n",
    "        decoded = pool.map(_openai_sample, prefixes)\n",
    "    else:\n",
    "        decoded = ['' for _ in range(len(texts))]\n",
    "\n",
    "        # sample from the model until we get a sample with at least min_words words for each example\n",
    "        # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works\n",
    "        tries = 0\n",
    "        while (m := min(len(x.split()) for x in decoded)) < min_words:\n",
    "            if tries != 0:\n",
    "                print()\n",
    "                print(f\"min words: {m}, needed {min_words}, regenerating (try {tries})\")\n",
    "\n",
    "            sampling_kwargs = {}\n",
    "            if args.do_top_p:\n",
    "                sampling_kwargs['top_p'] = args.top_p\n",
    "            elif args.do_top_k:\n",
    "                sampling_kwargs['top_k'] = args.top_k\n",
    "            min_length = 50 if args.dataset in ['pubmed'] else 150\n",
    "            outputs = base_model.generate(**all_encoded, min_length=min_length, max_length=200, do_sample=True, **sampling_kwargs, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)\n",
    "            decoded = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            tries += 1\n",
    "\n",
    "    if args.openai_model:\n",
    "        global API_TOKEN_COUNTER\n",
    "\n",
    "        # count total number of tokens with GPT2_TOKENIZER\n",
    "        total_tokens = sum(len(GPT2_TOKENIZER.encode(x)) for x in decoded)\n",
    "        API_TOKEN_COUNTER += total_tokens\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(logits, labels):\n",
    "    assert logits.shape[0] == 1\n",
    "    assert labels.shape[0] == 1\n",
    "\n",
    "    logits = logits.view(-1, logits.shape[-1])[:-1]\n",
    "    labels = labels.view(-1)[1:]\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    log_likelihood = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    return log_likelihood.mean()\n",
    "\n",
    "# Get the log likelihood of each text under the base_model\n",
    "def get_ll(text):\n",
    "    if args.openai_model:        \n",
    "        kwargs = { \"engine\": args.openai_model, \"temperature\": 0, \"max_tokens\": 0, \"echo\": True, \"logprobs\": 0}\n",
    "        r = openai.Completion.create(prompt=f\"<|endoftext|>{text}\", **kwargs)\n",
    "        result = r['choices'][0]\n",
    "        tokens, logprobs = result[\"logprobs\"][\"tokens\"][1:], result[\"logprobs\"][\"token_logprobs\"][1:]\n",
    "\n",
    "        assert len(tokens) == len(logprobs), f\"Expected {len(tokens)} logprobs, got {len(logprobs)}\"\n",
    "\n",
    "        return np.mean(logprobs)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "            labels = tokenized.input_ids\n",
    "            return -base_model(**tokenized, labels=labels).loss.item()\n",
    "        \n",
    "def get_lls(texts):\n",
    "    if not args.openai_model:\n",
    "        return [get_ll(text) for text in texts]\n",
    "    else:\n",
    "        global API_TOKEN_COUNTER\n",
    "\n",
    "        # use GPT2_TOKENIZER to get total number of tokens\n",
    "        total_tokens = sum(len(GPT2_TOKENIZER.encode(text)) for text in texts)\n",
    "        API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens\n",
    "\n",
    "        pool = ThreadPool(args.batch_size)\n",
    "        return pool.map(get_ll, texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anansi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
